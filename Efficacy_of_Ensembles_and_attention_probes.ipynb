{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-oFp9AxEvNyCFwN_r-c9AdJGzCzF8po5",
      "authorship_tag": "ABX9TyMLiIPep//VgVGXiBoFHNii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahadikprasad15/Efficacy-of-ensemble-of-attention-probes/blob/main/Efficacy_of_Ensembles_and_attention_probes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jiTfOc4oXFF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "%cd /content/drive/MyDrive\n",
        "\n",
        "# Clone repository (if not already cloned)\n",
        "!git clone https://github.com/mahadikprasad15/Efficacy-of-ensemble-of-attention-probes.git\n",
        "%cd Efficacy-of-ensemble-of-attention-probes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "qwc0-0iGtDOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision transformers safetensors pyyaml requests tqdm scikit-learn matplotlib pandas\n",
        "!pip install -q cerebras-cloud-sdk\n",
        "\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "f7eS5nFW1r-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# HuggingFace Token (for Llama 3.2)\n",
        "# Get from: https://huggingface.co/settings/tokens\n",
        "hf_token = getpass(\"Enter your HuggingFace token: \")\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "# Cerebras API Key (for labeling)\n",
        "# Get from: https://cloud.cerebras.ai/\n",
        "cerebras_key = getpass(\"Enter your Cerebras API key: \")\n",
        "os.environ['CEREBRAS_API_KEY'] = cerebras_key\n",
        "\n",
        "print(\"‚úì Tokens set!\")"
      ],
      "metadata": {
        "id": "SJsBA_O72AnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download roleplaying dataset\n",
        "!python scripts/download_apollo_data.py \\\n",
        "    --datasets roleplaying \\\n",
        "    --output_dir data/apollo_raw\n",
        "\n",
        "# Verify download\n",
        "!ls -lh data/apollo_raw/roleplaying/\n",
        "\n",
        "# Optional: Preview the dataset\n",
        "import yaml\n",
        "with open('data/apollo_raw/roleplaying/dataset.yaml', 'r') as f:\n",
        "    data = yaml.safe_load(f)\n",
        "    print(f\"Total scenarios: {len(data)}\")\n",
        "    print(\"\\nFirst scenario example:\")\n",
        "    print(f\"Scenario: {data[0]['scenario'][:200]}...\")\n",
        "    print(f\"Question: {data[0]['question']}\")\n",
        "    print(f\"Answer prefix: {data[0]['answer_prefix']}\")\n"
      ],
      "metadata": {
        "id": "Abjm3Jdn2LC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache training set (100 examples for quick testing)\n",
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --split train \\\n",
        "    --limit 100 \\\n",
        "    --batch_size 4 \\\n",
        "    --L_prime 28 \\\n",
        "    --T_prime 64 \\\n",
        "    --hf_token $HF_TOKEN \\\n",
        "    --labeling_model llama3.1-8b \\\n",
        "    --requests_per_minute 25\n",
        "\n",
        "# This will:\n",
        "# - Load 100 scenarios\n",
        "# - Generate completions using Llama-3.2-3B (5-10 min)\n",
        "# - Label using Cerebras Llama-8B (4-5 min)\n",
        "# - Extract activations (only from generated tokens)\n",
        "# - Resample to (28, 64, 3072)\n",
        "# - Save to data/activations/...\n"
      ],
      "metadata": {
        "id": "urFm4UNk2Tov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/validate_deception_data.py \\\n",
        "    --activations_dir data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/train"
      ],
      "metadata": {
        "id": "8pgYhOuL2WT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train split (full dataset, ~180 examples)\n",
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --split train \\\n",
        "    --batch_size 4 \\\n",
        "    --hf_token $HF_TOKEN\n",
        "\n",
        "# Validation split\n",
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --split validation \\\n",
        "    --batch_size 4 \\\n",
        "    --hf_token $HF_TOKEN\n",
        "\n",
        "# Test split\n",
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --split test \\\n",
        "    --batch_size 4 \\\n",
        "    --hf_token $HF_TOKEN\n",
        "\n"
      ],
      "metadata": {
        "id": "1CuN892g4i15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the cached activations\n",
        "!python scripts/validate_deception_data.py \\\n",
        "    --activations_dir data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/train\n",
        "\n",
        "# Check what was saved\n",
        "!ls -lh data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/train/\n",
        "\n",
        "# Preview manifest\n",
        "!head -n 3 data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/train/manifest.jsonl"
      ],
      "metadata": {
        "id": "a8maeQWy82dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train mean pooling probes on all layers\n",
        "!python scripts/train_deception_probes.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --pooling mean \\\n",
        "    --batch_size 32 \\\n",
        "    --epochs 10 \\\n",
        "    --patience 5 \\\n",
        "    --lr 0.001 \\\n",
        "    --weight_decay 0.0001\n",
        "\n",
        "# Probes saved to:\n",
        "# data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/"
      ],
      "metadata": {
        "id": "VR62ON6lBe0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze mean pooling results\n",
        "!python scripts/analyze_probes.py \\\n",
        "    --probes_dir data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean \\\n",
        "    --save_plots \\\n",
        "    --save_report\n",
        "\n",
        "# View analysis report\n",
        "!cat data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/analysis_report.txt\n",
        "\n",
        "# View best probe info\n",
        "!cat data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/best_probe.json\n",
        "\n",
        "# Display the per-layer AUC plot\n",
        "from IPython.display import Image, display\n",
        "display(Image('data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/per_layer_analysis.png'))\n"
      ],
      "metadata": {
        "id": "0fBAnIcXCkih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate best probe on test split\n",
        "!python scripts/eval_ood.py \\\n",
        "    --best_probe_json data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/best_probe.json \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --eval_dataset Deception-Roleplaying \\\n",
        "    --eval_split test\n",
        "\n",
        "# View test results\n",
        "!cat data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/eval_Deception-Roleplaying_test.json\n"
      ],
      "metadata": {
        "id": "In-HVL34GKHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Max pooling\n",
        "!python scripts/train_deception_probes.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --pooling max \\\n",
        "    --epochs 10 \\\n",
        "    --batch_size 32\n",
        "\n",
        "\n",
        "# Last token pooling\n",
        "!python scripts/train_deception_probes.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --pooling last \\\n",
        "    --epochs 10 \\\n",
        "    --batch_size 32\n",
        "\n",
        "# Attention pooling (learned)\n",
        "!python scripts/train_deception_probes.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --pooling attn \\\n",
        "    --batch_size 32 \\\n",
        "    --epochs 20\n"
      ],
      "metadata": {
        "id": "1q3a0YELIFZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Re-run compare_results.py with verbose output\n",
        "# ============================================================================\n",
        "\n",
        "!python scripts/compare_results.py \\\n",
        "    --experiments_dir data/probes \\\n",
        "    --output_dir results/comparisons \\\n",
        "    --save_csv 2>&1 | tee compare_output.txt\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìÅ Files generated:\")\n",
        "!ls -la results/comparisons/\n",
        "\n"
      ],
      "metadata": {
        "id": "uXbcbyWnIXhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Generate Layerwise Comparison Plot (Inline - No Script Needed)\n",
        "# ============================================================================\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PROBES_BASE = \"data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying\"\n",
        "OUTPUT_DIR = \"results/comparisons\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load all pooling results\n",
        "colors = {\n",
        "    'mean': '#2E86AB',\n",
        "    'max': '#A23B72',\n",
        "    'last': '#F18F01',\n",
        "    'attn': '#06A77D'\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    results_file = f\"{PROBES_BASE}/{pooling}/layer_results.json\"\n",
        "    if os.path.exists(results_file):\n",
        "        with open(results_file, 'r') as f:\n",
        "            all_results[pooling] = json.load(f)\n",
        "        print(f\"‚úì Loaded {pooling}\")\n",
        "\n",
        "# Check if accuracy data exists\n",
        "has_accuracy = all_results and 'val_acc' in all_results[list(all_results.keys())[0]][0]\n",
        "\n",
        "# Create figure\n",
        "if has_accuracy:\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
        "else:\n",
        "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
        "    ax2 = None\n",
        "\n",
        "overall_best_auc = 0\n",
        "overall_best_info = None\n",
        "\n",
        "# Plot each pooling strategy\n",
        "for pooling, layer_results in all_results.items():\n",
        "    layers = [r['layer'] for r in layer_results]\n",
        "    aucs = [r['val_auc'] for r in layer_results]\n",
        "    color = colors.get(pooling, '#666666')\n",
        "\n",
        "    # Plot AUC\n",
        "    ax1.plot(layers, aucs, marker='o', linewidth=2.5, markersize=6,\n",
        "             color=color, label=f'{pooling.upper()}', alpha=0.85)\n",
        "\n",
        "    # Mark best layer\n",
        "    best = max(layer_results, key=lambda x: x['val_auc'])\n",
        "    ax1.scatter([best['layer']], [best['val_auc']],\n",
        "                color=color, s=200, zorder=5, edgecolors='black',\n",
        "                linewidths=2.5, marker='*')\n",
        "\n",
        "    # Track overall best\n",
        "    if best['val_auc'] > overall_best_auc:\n",
        "        overall_best_auc = best['val_auc']\n",
        "        overall_best_info = (pooling, best['layer'], best['val_auc'])\n",
        "\n",
        "    # Plot accuracy if available\n",
        "    if ax2 is not None and 'val_acc' in layer_results[0]:\n",
        "        accs = [r.get('val_acc', 0.5) for r in layer_results]\n",
        "        ax2.plot(layers, accs, marker='s', linewidth=2.5, markersize=6,\n",
        "                 color=color, label=f'{pooling.upper()}', alpha=0.85)\n",
        "\n",
        "# Annotate overall best\n",
        "if overall_best_info:\n",
        "    pooling, layer, auc = overall_best_info\n",
        "    color = colors.get(pooling, '#666666')\n",
        "    ax1.annotate(\n",
        "        f'BEST: {pooling.upper()}\\nLayer {layer}\\nAUC: {auc:.3f}',\n",
        "        xy=(layer, auc),\n",
        "        xytext=(15, 15),\n",
        "        textcoords='offset points',\n",
        "        bbox=dict(boxstyle='round,pad=0.8', facecolor=color, alpha=0.3,\n",
        "                 edgecolor='black', linewidth=2),\n",
        "        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3',\n",
        "                       color='black', lw=2),\n",
        "        fontsize=11, fontweight='bold', ha='left'\n",
        "    )\n",
        "\n",
        "# Style AUC plot\n",
        "ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.4, linewidth=1.5, label='Random')\n",
        "ax1.axhline(y=0.7, color='green', linestyle=':', alpha=0.4, linewidth=1.5, label='Strong (0.7)')\n",
        "ax1.set_ylabel('Validation AUC', fontsize=13, fontweight='bold')\n",
        "ax1.set_title('Layerwise Validation AUC Comparison\\nAll 4 Pooling Strategies', fontsize=14, fontweight='bold')\n",
        "ax1.legend(loc='best', fontsize=11, framealpha=0.9)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "ax1.set_ylim(0.45, 1.0)\n",
        "\n",
        "# Style accuracy plot if present\n",
        "if ax2 is not None:\n",
        "    ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.4, linewidth=1.5)\n",
        "    ax2.set_xlabel('Layer', fontsize=13, fontweight='bold')\n",
        "    ax2.set_ylabel('Validation Accuracy', fontsize=13, fontweight='bold')\n",
        "    ax2.set_title('Layerwise Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(loc='best', fontsize=11, framealpha=0.9)\n",
        "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax2.set_ylim(0.45, 1.0)\n",
        "else:\n",
        "    ax1.set_xlabel('Layer', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save and display\n",
        "save_path = f\"{OUTPUT_DIR}/layerwise_pooling_comparison.png\"\n",
        "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"\\n‚úì Saved: {save_path}\")\n",
        "\n",
        "# Display\n",
        "from IPython.display import Image, display\n",
        "display(Image(save_path, width=900))\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä Summary: Best Layer for Each Pooling Strategy\")\n",
        "print(\"=\" * 60)\n",
        "for pooling, layer_results in all_results.items():\n",
        "    best = max(layer_results, key=lambda x: x['val_auc'])\n",
        "    marker = \" ‚≠ê BEST\" if overall_best_info and pooling == overall_best_info[0] else \"\"\n",
        "    print(f\"  {pooling.upper():6s}: Layer {best['layer']:2d} | AUC: {best['val_auc']:.4f}{marker}\")"
      ],
      "metadata": {
        "id": "gBtBpVTr7zrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and visualize per-layer results\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load results\n",
        "with open('data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/layer_results.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "# Extract data\n",
        "layers = [r['layer'] for r in results]\n",
        "aucs = [r['val_auc'] for r in results]\n",
        "epochs = [r['epoch'] for r in results]\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: AUC per layer\n",
        "ax1.plot(layers, aucs, marker='o', linewidth=2, markersize=6, color='#2E86AB')\n",
        "ax1.axhline(y=0.5, color='red', linestyle='--', label='Random Chance', alpha=0.5)\n",
        "best_layer = max(results, key=lambda x: x['val_auc'])\n",
        "ax1.scatter([best_layer['layer']], [best_layer['val_auc']],\n",
        "            color='orange', s=200, zorder=5, label=f\"Best: Layer {best_layer['layer']}\")\n",
        "ax1.set_xlabel('Layer', fontsize=12)\n",
        "ax1.set_ylabel('Validation AUC', fontsize=12)\n",
        "ax1.set_title('Deception Detection by Layer', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: Training epochs per layer\n",
        "ax2.bar(layers, epochs, alpha=0.7, color='#A23B72')\n",
        "ax2.set_xlabel('Layer', fontsize=12)\n",
        "ax2.set_ylabel('Training Epochs', fontsize=12)\n",
        "ax2.set_title('Early Stopping Epochs by Layer', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('custom_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Best Layer: {best_layer['layer']} (AUC: {best_layer['val_auc']:.4f})\")\n"
      ],
      "metadata": {
        "id": "b_ik3wKYN-UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Efficacy-of-ensemble-of-attention-probes\n",
        "!git pull origin main"
      ],
      "metadata": {
        "id": "OA7CH0ofOK_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear old data\n",
        "!rm -rf data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/\n",
        "\n",
        "# Re-cache\n",
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-InsiderTrading \\\n",
        "    --split test \\\n",
        "    --limit 200 \\\n",
        "    --batch_size 4 \\\n",
        "    --hf_token $HF_TOKEN"
      ],
      "metadata": {
        "id": "2PnW9mMMuBxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O4: Validate OOD Activations - Should Show Balanced Labels\n",
        "# ============================================================================\n",
        "!python scripts/validate_deception_data.py \\\n",
        "    --activations_dir data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/test"
      ],
      "metadata": {
        "id": "UpcWF9Iq9q7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O5: Evaluate Probes on OOD - INLINE VERSION (Bypasses Loader Bug)\n",
        "# ============================================================================\n",
        "!git pull origin main\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from safetensors.torch import load_file\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.path.join(os.getcwd(), 'actprobe', 'src'))\n",
        "from actprobe.probes.models import LayerProbe\n",
        "\n",
        "# Paths\n",
        "OOD_DIR = \"data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/test\"\n",
        "PROBES_BASE = \"data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying\"\n",
        "OUTPUT_DIR = \"results/ood_evaluation\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load OOD data\n",
        "print(\"Loading OOD data...\")\n",
        "with open(f\"{OOD_DIR}/manifest.jsonl\", 'r') as f:\n",
        "    manifest = [json.loads(line) for line in f]\n",
        "\n",
        "# Load all shards\n",
        "shards = sorted(glob.glob(f\"{OOD_DIR}/shard_*.safetensors\"))\n",
        "all_tensors = {}\n",
        "for shard_path in shards:\n",
        "    all_tensors.update(load_file(shard_path))\n",
        "\n",
        "samples = []\n",
        "labels = []\n",
        "for entry in manifest:\n",
        "    eid = entry['id']\n",
        "    if eid in all_tensors:\n",
        "        samples.append(all_tensors[eid])\n",
        "        labels.append(entry['label'])\n",
        "\n",
        "X = torch.stack(samples).float()\n",
        "y = np.array(labels)\n",
        "print(f\"‚úì Loaded {len(X)} OOD samples\")\n",
        "print(f\"  Labels: {sum(y==0)} honest, {sum(y==1)} deceptive\")\n",
        "\n",
        "# Colors\n",
        "COLORS = {'mean': '#2E86AB', 'max': '#A23B72', 'last': '#F18F01', 'attn': '#06A77D'}\n",
        "\n",
        "# Evaluate all pooling strategies\n",
        "all_results = {}\n",
        "\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    probe_dir = f\"{PROBES_BASE}/{pooling}\"\n",
        "    probe_files = sorted(glob.glob(f\"{probe_dir}/probe_layer_*.pt\"))\n",
        "\n",
        "    if not probe_files:\n",
        "        print(f\"‚ö†Ô∏è No probes for {pooling}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nEvaluating {pooling.upper()} ({len(probe_files)} layers)...\")\n",
        "\n",
        "    D = X.shape[-1]\n",
        "    layer_results = []\n",
        "\n",
        "    for pf in tqdm(probe_files, desc=pooling):\n",
        "        layer_idx = int(pf.split('_')[-1].replace('.pt', ''))\n",
        "\n",
        "        probe = LayerProbe(input_dim=D, pooling_type=pooling).to(device)\n",
        "        probe.load_state_dict(torch.load(pf, map_location=device))\n",
        "        probe.eval()\n",
        "\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(X), 16):\n",
        "                batch = X[i:i+16, layer_idx, :, :].to(device)\n",
        "                logits = probe(batch)\n",
        "                probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "                preds.extend(probs)\n",
        "\n",
        "        preds = np.array(preds)\n",
        "        try:\n",
        "            auc = roc_auc_score(y, preds)\n",
        "        except:\n",
        "            auc = 0.5\n",
        "        acc = accuracy_score(y, (preds > 0.5).astype(int))\n",
        "\n",
        "        layer_results.append({'layer': layer_idx, 'auc': auc, 'acc': acc})\n",
        "\n",
        "    best = max(layer_results, key=lambda x: x['auc'])\n",
        "    all_results[pooling] = {\n",
        "        'layers': [r['layer'] for r in layer_results],\n",
        "        'aucs': [r['auc'] for r in layer_results],\n",
        "        'accs': [r['acc'] for r in layer_results],\n",
        "        'best_layer': best['layer'],\n",
        "        'best_auc': best['auc']\n",
        "    }\n",
        "    print(f\"  Best: Layer {best['layer']} | AUC: {best['auc']:.4f}\")\n",
        "\n",
        "# Save results\n",
        "with open(f\"{OUTPUT_DIR}/ood_results_all_pooling.json\", 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "for pooling, res in all_results.items():\n",
        "    color = COLORS[pooling]\n",
        "    ax.plot(res['layers'], res['aucs'], marker='o', linewidth=2.5,\n",
        "            color=color, label=pooling.upper(), alpha=0.85)\n",
        "    ax.scatter([res['best_layer']], [res['best_auc']],\n",
        "               color=color, s=200, zorder=5, edgecolors='black',\n",
        "               linewidths=2.5, marker='*')\n",
        "\n",
        "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.4, label='Random')\n",
        "ax.set_xlabel('Layer', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('OOD AUC', fontsize=13, fontweight='bold')\n",
        "ax.set_title('OOD Evaluation: Insider Trading\\nAll Pooling Strategies', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=11)\n",
        "ax.grid(True, alpha=0.3, linestyle='--')\n",
        "ax.set_ylim(0.4, 1.0)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_path = f\"{OUTPUT_DIR}/ood_layerwise_comparison.png\"\n",
        "plt.savefig(save_path, dpi=300)\n",
        "print(f\"\\n‚úì Saved: {save_path}\")\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(save_path, width=900))\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä OOD EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "best_overall = max(all_results.items(), key=lambda x: x[1]['best_auc'])\n",
        "for pooling, res in all_results.items():\n",
        "    marker = \" ‚≠ê\" if pooling == best_overall[0] else \"\"\n",
        "    print(f\"  {pooling.upper():6s}: Layer {res['best_layer']:2d} | AUC: {res['best_auc']:.4f}{marker}\")"
      ],
      "metadata": {
        "id": "h5tZbPUbFcKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O6: Display OOD Results\n",
        "# ============================================================================\n",
        "from IPython.display import Image, display\n",
        "\n",
        "OOD_RESULTS = \"results/ood_evaluation\"\n",
        "\n",
        "# Display comparison plot (all 4 pooling strategies on OOD)\n",
        "plot_path = f\"{OOD_RESULTS}/ood_layerwise_comparison.png\"\n",
        "print(\"üìä OOD Layerwise Comparison (All 4 Pooling Strategies):\")\n",
        "display(Image(plot_path, width=800))\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nüìã OOD Best Probes Summary:\")\n",
        "!cat {OOD_RESULTS}/ood_best_probes_summary.txt"
      ],
      "metadata": {
        "id": "cnWgIHK-9x4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the fix\n",
        "!git pull origin main\n",
        "\n",
        "# Delete old results (so it doesn't skip)\n",
        "!rm -rf results/ensembles/*/ensemble_k_sweep_*.json\n",
        "\n",
        "# Re-run ensemble evaluation\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä Ensemble evaluation: {pooling.upper()} pooling (Validation)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    !python scripts/evaluate_ensembles_comprehensive.py \\\n",
        "        --pooling {pooling} \\\n",
        "        --val_activations_dir data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation \\\n",
        "        --probes_dir data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/{pooling} \\\n",
        "        --output_dir results/ensembles/{pooling}"
      ],
      "metadata": {
        "id": "qk8Ubd5E_Kwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O7: Ensemble K-Sweep on Validation Set (All Pooling Strategies)\n",
        "# ============================================================================\n",
        "\n",
        "VAL_ACTIVATIONS = \"data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation\"\n",
        "PROBES_BASE = \"data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying\"\n",
        "ENSEMBLE_RESULTS = \"results/ensembles\"\n",
        "K_VALUES = \"10,20,30,40,50,60,70,80,90\"\n",
        "\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä Ensemble evaluation: {pooling.upper()} pooling (Validation)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    !python scripts/evaluate_ensembles_comprehensive.py \\\n",
        "        --pooling {pooling} \\\n",
        "        --val_activations_dir {VAL_ACTIVATIONS} \\\n",
        "        --probes_dir {PROBES_BASE}/{pooling} \\\n",
        "        --output_dir {ENSEMBLE_RESULTS}/{pooling} \\\n",
        "        --eval_mode validation \\\n",
        "        --k_values {K_VALUES}\n",
        "\n",
        "print(\"\\n‚úÖ Validation ensemble evaluation complete!\")"
      ],
      "metadata": {
        "id": "C7Np6ERk_UQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O7.5: Extract OOD Logits for Ensemble Evaluation\n",
        "# ============================================================================\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from safetensors.torch import load_file\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.path.join(os.getcwd(), 'actprobe', 'src'))\n",
        "from actprobe.probes.models import LayerProbe\n",
        "\n",
        "# Paths\n",
        "OOD_DIR = \"data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/test\"\n",
        "PROBES_BASE = \"data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying\"\n",
        "OUTPUT_DIR = \"results/ood_evaluation/logits\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load OOD data\n",
        "print(\"Loading OOD data...\")\n",
        "with open(f\"{OOD_DIR}/manifest.jsonl\", 'r') as f:\n",
        "    manifest = [json.loads(line) for line in f]\n",
        "\n",
        "shards = sorted(glob.glob(f\"{OOD_DIR}/shard_*.safetensors\"))\n",
        "all_tensors = {}\n",
        "for shard_path in shards:\n",
        "    all_tensors.update(load_file(shard_path))\n",
        "\n",
        "samples = []\n",
        "labels = []\n",
        "for entry in manifest:\n",
        "    eid = entry['id']\n",
        "    if eid in all_tensors:\n",
        "        samples.append(all_tensors[eid])\n",
        "        labels.append(entry['label'])\n",
        "\n",
        "X = torch.stack(samples).float()\n",
        "y = np.array(labels)\n",
        "print(f\"‚úì Loaded {len(X)} OOD samples\")\n",
        "print(f\"  Labels: {sum(y==0)} honest, {sum(y==1)} deceptive\")\n",
        "\n",
        "# Save labels\n",
        "np.save(f\"{OUTPUT_DIR}/labels.npy\", y)\n",
        "print(f\"‚úì Saved: {OUTPUT_DIR}/labels.npy\")\n",
        "\n",
        "# Extract logits for each pooling strategy\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    probe_dir = f\"{PROBES_BASE}/{pooling}\"\n",
        "\n",
        "    # Sort numerically!\n",
        "    probe_files = sorted(\n",
        "        glob.glob(f\"{probe_dir}/probe_layer_*.pt\"),\n",
        "        key=lambda x: int(x.split('_')[-1].replace('.pt', ''))\n",
        "    )\n",
        "\n",
        "    if not probe_files:\n",
        "        print(f\"‚ö†Ô∏è No probes for {pooling}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nExtracting {pooling.upper()} logits ({len(probe_files)} layers)...\")\n",
        "\n",
        "    D = X.shape[-1]\n",
        "    all_layer_logits = []\n",
        "\n",
        "    for pf in tqdm(probe_files, desc=pooling):\n",
        "        layer_idx = int(pf.split('_')[-1].replace('.pt', ''))\n",
        "\n",
        "        probe = LayerProbe(input_dim=D, pooling_type=pooling).to(device)\n",
        "        probe.load_state_dict(torch.load(pf, map_location=device))\n",
        "        probe.eval()\n",
        "\n",
        "        layer_logits = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(X), 16):\n",
        "                batch = X[i:i+16, layer_idx, :, :].to(device)\n",
        "                logits = probe(batch).cpu().numpy().flatten()\n",
        "                layer_logits.extend(logits)\n",
        "\n",
        "        all_layer_logits.append(np.array(layer_logits))\n",
        "\n",
        "    # Save as (N, L) array\n",
        "    logits_array = np.array(all_layer_logits).T\n",
        "    save_path = f\"{OUTPUT_DIR}/{pooling}_logits.npy\"\n",
        "    np.save(save_path, logits_array)\n",
        "    print(f\"  ‚úì Saved: {save_path} {logits_array.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ OOD LOGITS EXTRACTION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Files saved to: {OUTPUT_DIR}/\")\n",
        "print(\"  - labels.npy\")\n",
        "print(\"  - mean_logits.npy\")\n",
        "print(\"  - max_logits.npy\")\n",
        "print(\"  - last_logits.npy\")\n",
        "print(\"  - attn_logits.npy\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚û°Ô∏è Now run Cell O8 for OOD ensemble evaluation!\")"
      ],
      "metadata": {
        "id": "GkFU_PVUPiNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O8: Ensemble K-Sweep on OOD Set (All Pooling Strategies)\n",
        "# ============================================================================\n",
        "\n",
        "OOD_LOGITS_DIR = \"results/ood_evaluation/logits\"\n",
        "\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä Ensemble evaluation: {pooling.upper()} pooling (OOD)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    logits_path = f\"{OOD_LOGITS_DIR}/{pooling}_logits.npy\"\n",
        "    labels_path = f\"{OOD_LOGITS_DIR}/labels.npy\"\n",
        "\n",
        "    !python scripts/evaluate_ensembles_comprehensive.py \\\n",
        "        --pooling {pooling} \\\n",
        "        --probes_dir {PROBES_BASE}/{pooling} \\\n",
        "        --ood_logits_path {logits_path} \\\n",
        "        --ood_labels_path {labels_path} \\\n",
        "        --output_dir {ENSEMBLE_RESULTS}/{pooling} \\\n",
        "        --eval_mode ood \\\n",
        "        --k_values {K_VALUES}\n",
        "\n",
        "print(\"\\n‚úÖ OOD ensemble evaluation complete!\")"
      ],
      "metadata": {
        "id": "EMdoLy1B71UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O9: Final Cross-Pooling √ó Ensemble Comparison\n",
        "# ============================================================================\n",
        "\n",
        "FINAL_COMPARISON = \"results/final_comparison\"\n",
        "\n",
        "!python scripts/compare_all_pooling_ensembles.py \\\n",
        "    --results_dir {ENSEMBLE_RESULTS} \\\n",
        "    --output_dir {FINAL_COMPARISON} \\\n",
        "    --eval_type both\n",
        "\n",
        "print(f\"\\n‚úÖ Final comparison saved to: {FINAL_COMPARISON}\")"
      ],
      "metadata": {
        "id": "lgcfd-6173Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O10: Display All Final Visualizations\n",
        "# ============================================================================\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "FINAL_DIR = \"results/final_comparison\"\n",
        "\n",
        "# 1. Heatmaps: Pooling √ó Ensemble\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä POOLING √ó ENSEMBLE HEATMAPS (Best AUC)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for eval_type in ['validation', 'ood']:\n",
        "    heatmap = f\"{FINAL_DIR}/pooling_ensemble_heatmap_{eval_type}.png\"\n",
        "    if os.path.exists(heatmap):\n",
        "        print(f\"\\n{eval_type.upper()} Set:\")\n",
        "        display(Image(heatmap, width=600))\n",
        "\n",
        "# 2. Optimal K% Analysis\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä OPTIMAL K% ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for eval_type in ['validation', 'ood']:\n",
        "    k_plot = f\"{FINAL_DIR}/optimal_k_analysis_{eval_type}.png\"\n",
        "    if os.path.exists(k_plot):\n",
        "        print(f\"\\n{eval_type.upper()} Set:\")\n",
        "        display(Image(k_plot, width=800))\n",
        "\n",
        "# 3. Per-Ensemble Comparison\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä PER-ENSEMBLE COMPARISON (All Pooling Strategies)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for ensemble in ['mean', 'weighted', 'gated']:\n",
        "    for eval_type in ['validation', 'ood']:\n",
        "        plot = f\"{FINAL_DIR}/{ensemble}_comparison_{eval_type}.png\"\n",
        "        if os.path.exists(plot):\n",
        "            print(f\"\\n{ensemble.capitalize()} Ensemble - {eval_type.upper()}:\")\n",
        "            display(Image(plot, width=900))\n",
        "\n",
        "# 4. Final Summaries\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìã FINAL SUMMARIES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for eval_type in ['validation', 'ood']:\n",
        "    summary = f\"{FINAL_DIR}/final_summary_{eval_type}.txt\"\n",
        "    if os.path.exists(summary):\n",
        "        print(f\"\\n--- {eval_type.upper()} ---\")\n",
        "        !cat {summary}"
      ],
      "metadata": {
        "id": "X_TdjfeC75G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O11: (Optional) PCA Visualization - ID vs OOD\n",
        "# ============================================================================\n",
        "\n",
        "!python scripts/analysis/analyze_pca.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --compare_dataset Deception-InsiderTrading \\\n",
        "    --data_dir data/activations \\\n",
        "    --output_dir results/pca \\\n",
        "    --layer 20 \\\n",
        "    --pooling mean\n",
        "\n",
        "# Display PCA plots\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "pca_dir = \"results/pca\"\n",
        "for f in sorted(os.listdir(pca_dir)):\n",
        "    if f.endswith('.png') or f.endswith('.pdf'):\n",
        "        print(f\"\\nüìä {f}:\")\n",
        "        display(Image(f\"{pca_dir}/{f}\", width=600))"
      ],
      "metadata": {
        "id": "6-yf1HB6760F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the script\n",
        "!git pull origin main\n",
        "\n",
        "# Run the analysis\n",
        "!python scripts/analyze_mechanisms.py"
      ],
      "metadata": {
        "id": "hBYfboY-78Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git pull origin main\n",
        "!python scripts/analyze_attention_text.py\n"
      ],
      "metadata": {
        "id": "REcuqDj-7-Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "with open('results/mechanistic_analysis/attention_on_text.html') as f:\n",
        "    display(HTML(f.read()))"
      ],
      "metadata": {
        "id": "gHjfmr6Q8AJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main\n",
        "!python scripts/analyze_attention_text.py"
      ],
      "metadata": {
        "id": "4fRIQpEWNLLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "with open('results/mechanistic_analysis/attention_on_text.html') as f:\n",
        "    display(HTML(f.read()))"
      ],
      "metadata": {
        "id": "RzrR4JKiPHNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main\n",
        "!python scripts/analyze_ensemble_attention.py"
      ],
      "metadata": {
        "id": "bnXId38EQKWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "with open('results/mechanistic_analysis/ensemble_attention.html') as f:\n",
        "    display(HTML(f.read()))"
      ],
      "metadata": {
        "id": "NO0wt140TQUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Update and install the robust Chromium environment\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-browser chromium-chromedriver\n",
        "\n",
        "# 2. Install Selenium (the direct controller)\n",
        "!pip install selenium -q"
      ],
      "metadata": {
        "id": "ClvnKT5tTmn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Your Drive Path\n",
        "DRIVE_DIR = '/content/drive/MyDrive/Efficacy-of-ensemble-of-attention-probes/results/mechanistic_analysis'\n",
        "INPUT_FILE = 'ensemble_attention.html'\n",
        "OUTPUT_FILE = 'ensemble_attention.png'\n",
        "\n",
        "# Paths\n",
        "input_path = os.path.join(DRIVE_DIR, INPUT_FILE)\n",
        "final_output_path = os.path.join(DRIVE_DIR, OUTPUT_FILE)\n",
        "temp_html_path = '/tmp/temp_viz.html' # Local temp path\n",
        "\n",
        "# --- EXECUTION ---\n",
        "def convert_html_to_png():\n",
        "    # 1. Verify Input\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"‚ùå Error: Input file not found at {input_path}\")\n",
        "        return\n",
        "\n",
        "    # 2. Copy to /tmp/ to ensure the browser can read it (Bypasses Drive permissions)\n",
        "    shutil.copy(input_path, temp_html_path)\n",
        "    print(f\"üìñ Copied HTML to local temp storage: {temp_html_path}\")\n",
        "\n",
        "    # 3. Configure Headless Chrome\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    # Force a large default window to prevent horizontal cramping\n",
        "    chrome_options.add_argument('--window-size=1200,800')\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    try:\n",
        "        print(\"üöÄ Launching Browser...\")\n",
        "        # Load the local file\n",
        "        driver.get(f'file://{temp_html_path}')\n",
        "\n",
        "        # Give it a moment to render fonts/styles\n",
        "        time.sleep(2)\n",
        "\n",
        "        # 4. SMART RESIZING (The Magic Step)\n",
        "        # We ask the browser \"How tall is this page really?\"\n",
        "        total_height = driver.execute_script(\"return document.body.parentNode.scrollHeight\")\n",
        "        print(f\"üìè Detected Content Height: {total_height}px\")\n",
        "\n",
        "        # Resize window to fit the whole thing\n",
        "        driver.set_window_size(1200, total_height + 100) # +100 padding\n",
        "\n",
        "        # 5. Capture\n",
        "        print(\"üì∏ Taking Screenshot...\")\n",
        "        driver.save_screenshot(final_output_path)\n",
        "\n",
        "        if os.path.exists(final_output_path):\n",
        "            print(f\"‚úÖ SUCCESS! Saved to: {final_output_path}\")\n",
        "        else:\n",
        "            print(\"‚ùå Error: Screenshot command finished but file is missing.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Runtime Error: {e}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "# Run it\n",
        "convert_html_to_png()"
      ],
      "metadata": {
        "id": "d5IWXTPlULdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main\n",
        "!python scripts/analyze_layer_colored_attention.py"
      ],
      "metadata": {
        "id": "1JmrR9weUY6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Your Drive Path\n",
        "DRIVE_DIR = '/content/drive/MyDrive/Efficacy-of-ensemble-of-attention-probes/results/mechanistic_analysis'\n",
        "INPUT_FILE = 'layer_colored_attention.html'\n",
        "OUTPUT_FILE = 'layer_colored_attention.png'\n",
        "\n",
        "# Paths\n",
        "input_path = os.path.join(DRIVE_DIR, INPUT_FILE)\n",
        "final_output_path = os.path.join(DRIVE_DIR, OUTPUT_FILE)\n",
        "temp_html_path = '/tmp/temp_viz.html' # Local temp path\n",
        "\n",
        "# --- EXECUTION ---\n",
        "def convert_html_to_png():\n",
        "    # 1. Verify Input\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"‚ùå Error: Input file not found at {input_path}\")\n",
        "        return\n",
        "\n",
        "    # 2. Copy to /tmp/ to ensure the browser can read it (Bypasses Drive permissions)\n",
        "    shutil.copy(input_path, temp_html_path)\n",
        "    print(f\"üìñ Copied HTML to local temp storage: {temp_html_path}\")\n",
        "\n",
        "    # 3. Configure Headless Chrome\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    # Force a large default window to prevent horizontal cramping\n",
        "    chrome_options.add_argument('--window-size=1200,800')\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    try:\n",
        "        print(\"üöÄ Launching Browser...\")\n",
        "        # Load the local file\n",
        "        driver.get(f'file://{temp_html_path}')\n",
        "\n",
        "        # Give it a moment to render fonts/styles\n",
        "        time.sleep(2)\n",
        "\n",
        "        # 4. SMART RESIZING (The Magic Step)\n",
        "        # We ask the browser \"How tall is this page really?\"\n",
        "        total_height = driver.execute_script(\"return document.body.parentNode.scrollHeight\")\n",
        "        print(f\"üìè Detected Content Height: {total_height}px\")\n",
        "\n",
        "        # Resize window to fit the whole thing\n",
        "        driver.set_window_size(1200, total_height + 100) # +100 padding\n",
        "\n",
        "        # 5. Capture\n",
        "        print(\"üì∏ Taking Screenshot...\")\n",
        "        driver.save_screenshot(final_output_path)\n",
        "\n",
        "        if os.path.exists(final_output_path):\n",
        "            print(f\"‚úÖ SUCCESS! Saved to: {final_output_path}\")\n",
        "        else:\n",
        "            print(\"‚ùå Error: Screenshot command finished but file is missing.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Runtime Error: {e}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "# Run it\n",
        "convert_html_to_png()"
      ],
      "metadata": {
        "id": "oUbjioerX5Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main\n",
        "!python scripts/analyze_hybrid_attention.py"
      ],
      "metadata": {
        "id": "1oFNVE-uYOjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Your Drive Path\n",
        "DRIVE_DIR = '/content/drive/MyDrive/Efficacy-of-ensemble-of-attention-probes/results/mechanistic_analysis'\n",
        "INPUT_FILE = 'hybrid_attention.html'\n",
        "OUTPUT_FILE = 'hybrid_attention.png'\n",
        "\n",
        "# Paths\n",
        "input_path = os.path.join(DRIVE_DIR, INPUT_FILE)\n",
        "final_output_path = os.path.join(DRIVE_DIR, OUTPUT_FILE)\n",
        "temp_html_path = '/tmp/temp_viz.html' # Local temp path\n",
        "\n",
        "# --- EXECUTION ---\n",
        "def convert_html_to_png():\n",
        "    # 1. Verify Input\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"‚ùå Error: Input file not found at {input_path}\")\n",
        "        return\n",
        "\n",
        "    # 2. Copy to /tmp/ to ensure the browser can read it (Bypasses Drive permissions)\n",
        "    shutil.copy(input_path, temp_html_path)\n",
        "    print(f\"üìñ Copied HTML to local temp storage: {temp_html_path}\")\n",
        "\n",
        "    # 3. Configure Headless Chrome\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    # Force a large default window to prevent horizontal cramping\n",
        "    chrome_options.add_argument('--window-size=1200,800')\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    try:\n",
        "        print(\"üöÄ Launching Browser...\")\n",
        "        # Load the local file\n",
        "        driver.get(f'file://{temp_html_path}')\n",
        "\n",
        "        # Give it a moment to render fonts/styles\n",
        "        time.sleep(2)\n",
        "\n",
        "        # 4. SMART RESIZING (The Magic Step)\n",
        "        # We ask the browser \"How tall is this page really?\"\n",
        "        total_height = driver.execute_script(\"return document.body.parentNode.scrollHeight\")\n",
        "        print(f\"üìè Detected Content Height: {total_height}px\")\n",
        "\n",
        "        # Resize window to fit the whole thing\n",
        "        driver.set_window_size(1200, total_height + 100) # +100 padding\n",
        "\n",
        "        # 5. Capture\n",
        "        print(\"üì∏ Taking Screenshot...\")\n",
        "        driver.save_screenshot(final_output_path)\n",
        "\n",
        "        if os.path.exists(final_output_path):\n",
        "            print(f\"‚úÖ SUCCESS! Saved to: {final_output_path}\")\n",
        "        else:\n",
        "            print(\"‚ùå Error: Screenshot command finished but file is missing.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Runtime Error: {e}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "# Run it\n",
        "convert_html_to_png()"
      ],
      "metadata": {
        "id": "pINeIh8MZm5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main\n",
        "!python scripts/compare_fixed_vs_gated.py"
      ],
      "metadata": {
        "id": "v08uKC4SZu-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main\n",
        "\n",
        "# Delete old (cheating) OOD results\n",
        "!rm -rf results/ensembles/attn/ensemble_k_sweep_ood.json\n",
        "!rm -rf results/ensembles/attn/ensemble_comparison_ood.png\n",
        "!rm -rf results/ensembles/attn/gated_models_ood/  # No longer needed\n",
        "\n",
        "# Step 1: Run validation (trains gated models)\n",
        "!python scripts/evaluate_ensembles_comprehensive.py \\\n",
        "    --pooling attn \\\n",
        "    --val_activations_dir data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation \\\n",
        "    --probes_dir data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/attn \\\n",
        "    --eval_mode validation \\\n",
        "    --output_dir results/ensembles/attn\n",
        "\n",
        "# Step 2: Run OOD (uses validation-trained gated - FAIR)\n",
        "\n",
        "!python scripts/evaluate_ensembles_comprehensive.py \\\n",
        "    --pooling attn \\\n",
        "    --ood_logits_path results/ood_evaluation/logits/attn_logits.npy \\\n",
        "    --ood_labels_path results/ood_evaluation/logits/labels.npy \\\n",
        "    --probes_dir data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/attn \\\n",
        "    --eval_mode ood \\\n",
        "    --output_dir results/ensembles/attn"
      ],
      "metadata": {
        "id": "m5E9czd1GChC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main\n",
        "!python scripts/analyze_gating_weights.py \\\n",
        "    --ood_logits results/ood_evaluation/logits/attn_logits.npy \\\n",
        "    --ood_labels results/ood_evaluation/logits/labels.npy"
      ],
      "metadata": {
        "id": "W-cfT2XzMf_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main\n",
        "\n",
        "!python scripts/analyze_gating_weights.py \\\n",
        "    --id_activations data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation \\\n",
        "    --ood_logits results/ood_evaluation/logits/attn_logits.npy \\\n",
        "    --ood_labels results/ood_evaluation/logits/labels.npy \\\n",
        "    --probes_dir data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/attn \\\n",
        "    --k_pct 40"
      ],
      "metadata": {
        "id": "Le81xpD6ZqSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main\n",
        "!python scripts/analyze_gating_weights.py \\\n",
        "    --id_activations data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation \\\n",
        "    --ood_logits results/ood_evaluation/logits/attn_logits.npy \\\n",
        "    --ood_labels results/ood_evaluation/logits/labels.npy"
      ],
      "metadata": {
        "id": "Q5VtTO8Rb6XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-InsiderTrading --split train \\\n",
        "    --limit 200 --use_pregenerated"
      ],
      "metadata": {
        "id": "9l58ob2sh7mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-InsiderTrading --split validation \\\n",
        "    --limit 80 --use_pregenerated"
      ],
      "metadata": {
        "id": "Izae5ZrMmZec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 2: Train ALL pooling probes\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    !python scripts/train_deception_probes.py \\\n",
        "        --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "        --dataset Deception-InsiderTrading \\\n",
        "        --pooling {pooling} \\\n",
        "        --output_dir data/probes_flipped \\\n",
        "        --epochs 10"
      ],
      "metadata": {
        "id": "8Z2fNtn1n1H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 3: OOD Evaluation\n",
        "# ============================================================================\n",
        "!python scripts/evaluate_ood_all_pooling.py \\\n",
        "    --ood_activations data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation \\\n",
        "    --probes_base data/probes_flipped/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading \\\n",
        "    --output_dir results_flipped/ood_evaluation"
      ],
      "metadata": {
        "id": "xJEUeku9n-oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PHASE 4: Ensemble K-Sweep (all pooling)\n",
        "# ============================================================================\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    !python scripts/evaluate_ensembles_comprehensive.py \\\n",
        "        --pooling {pooling} \\\n",
        "        --val_activations_dir data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/validation \\\n",
        "        --probes_dir data/probes_flipped/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/{pooling} \\\n",
        "        --eval_mode validation --output_dir results_flipped/ensembles/{pooling}\n",
        "    !python scripts/evaluate_ensembles_comprehensive.py \\\n",
        "        --pooling {pooling} \\\n",
        "        --ood_logits_path results_flipped/ood_evaluation/logits/{pooling}_logits.npy \\\n",
        "        --ood_labels_path results_flipped/ood_evaluation/logits/labels.npy \\\n",
        "        --probes_dir data/probes_flipped/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/{pooling} \\\n",
        "        --eval_mode ood --output_dir results_flipped/ensembles/{pooling}\n"
      ],
      "metadata": {
        "id": "Mphaopi-_aDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}