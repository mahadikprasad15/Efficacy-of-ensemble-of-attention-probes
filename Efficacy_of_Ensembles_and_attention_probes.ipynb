{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-oFp9AxEvNyCFwN_r-c9AdJGzCzF8po5",
      "authorship_tag": "ABX9TyN1/3aPcmh3uHL5EteqLT2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahadikprasad15/Efficacy-of-ensemble-of-attention-probes/blob/main/Efficacy_of_Ensembles_and_attention_probes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jiTfOc4oXFF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "%cd /content/drive/MyDrive\n",
        "\n",
        "# Clone repository (if not already cloned)\n",
        "!git clone https://github.com/mahadikprasad15/Efficacy-of-ensemble-of-attention-probes.git\n",
        "%cd Efficacy-of-ensemble-of-attention-probes\n",
        "\n",
        "# Pull latest changes (if repo already exists)\n",
        "!git checkout claude/plan-data-labeling-TVUmW\n",
        "!git pull origin claude/plan-data-labeling-TVUmW"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "qwc0-0iGtDOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision transformers safetensors pyyaml requests tqdm scikit-learn matplotlib pandas\n",
        "!pip install -q cerebras-cloud-sdk\n",
        "\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "f7eS5nFW1r-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# HuggingFace Token (for Llama 3.2)\n",
        "# Get from: https://huggingface.co/settings/tokens\n",
        "hf_token = getpass(\"Enter your HuggingFace token: \")\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "# Cerebras API Key (for labeling)\n",
        "# Get from: https://cloud.cerebras.ai/\n",
        "cerebras_key = getpass(\"Enter your Cerebras API key: \")\n",
        "os.environ['CEREBRAS_API_KEY'] = cerebras_key\n",
        "\n",
        "print(\"‚úì Tokens set!\")\n"
      ],
      "metadata": {
        "id": "SJsBA_O72AnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download roleplaying dataset\n",
        "!python scripts/download_apollo_data.py \\\n",
        "    --datasets roleplaying \\\n",
        "    --output_dir data/apollo_raw\n",
        "\n",
        "# Verify download\n",
        "!ls -lh data/apollo_raw/roleplaying/\n",
        "\n",
        "# Optional: Preview the dataset\n",
        "import yaml\n",
        "with open('data/apollo_raw/roleplaying/dataset.yaml', 'r') as f:\n",
        "    data = yaml.safe_load(f)\n",
        "    print(f\"Total scenarios: {len(data)}\")\n",
        "    print(\"\\nFirst scenario example:\")\n",
        "    print(f\"Scenario: {data[0]['scenario'][:200]}...\")\n",
        "    print(f\"Question: {data[0]['question']}\")\n",
        "    print(f\"Answer prefix: {data[0]['answer_prefix']}\")\n"
      ],
      "metadata": {
        "id": "Abjm3Jdn2LC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache training set (100 examples for quick testing)\n",
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --split train \\\n",
        "    --limit 100 \\\n",
        "    --batch_size 4 \\\n",
        "    --L_prime 28 \\\n",
        "    --T_prime 64 \\\n",
        "    --hf_token $HF_TOKEN \\\n",
        "    --labeling_model llama3.1-8b \\\n",
        "    --requests_per_minute 25\n",
        "\n",
        "# This will:\n",
        "# - Load 100 scenarios\n",
        "# - Generate completions using Llama-3.2-3B (5-10 min)\n",
        "# - Label using Cerebras Llama-8B (4-5 min)\n",
        "# - Extract activations (only from generated tokens)\n",
        "# - Resample to (28, 64, 3072)\n",
        "# - Save to data/activations/...\n"
      ],
      "metadata": {
        "id": "urFm4UNk2Tov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/validate_deception_data.py \\\n",
        "    --activations_dir data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/train"
      ],
      "metadata": {
        "id": "8pgYhOuL2WT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train split (full dataset, ~180 examples)\n",
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --split train \\\n",
        "    --batch_size 4 \\\n",
        "    --hf_token $HF_TOKEN\n",
        "\n",
        "# Validation split\n",
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --split validation \\\n",
        "    --batch_size 4 \\\n",
        "    --hf_token $HF_TOKEN\n",
        "\n",
        "# Test split\n",
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --split test \\\n",
        "    --batch_size 4 \\\n",
        "    --hf_token $HF_TOKEN\n",
        "\n"
      ],
      "metadata": {
        "id": "1CuN892g4i15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the cached activations\n",
        "!python scripts/validate_deception_data.py \\\n",
        "    --activations_dir data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/train\n",
        "\n",
        "# Check what was saved\n",
        "!ls -lh data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/train/\n",
        "\n",
        "# Preview manifest\n",
        "!head -n 3 data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/train/manifest.jsonl\n"
      ],
      "metadata": {
        "id": "a8maeQWy82dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train mean pooling probes on all layers\n",
        "!python scripts/train_deception_probes.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --pooling mean \\\n",
        "    --batch_size 32 \\\n",
        "    --epochs 10 \\\n",
        "    --patience 5 \\\n",
        "    --lr 0.001 \\\n",
        "    --weight_decay 0.0001\n",
        "\n",
        "# Probes saved to:\n",
        "# data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/\n"
      ],
      "metadata": {
        "id": "VR62ON6lBe0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze mean pooling results\n",
        "!python scripts/analyze_probes.py \\\n",
        "    --probes_dir data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean \\\n",
        "    --save_plots \\\n",
        "    --save_report\n",
        "\n",
        "# View analysis report\n",
        "!cat data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/analysis_report.txt\n",
        "\n",
        "# View best probe info\n",
        "!cat data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/best_probe.json\n",
        "\n",
        "# Display the per-layer AUC plot\n",
        "from IPython.display import Image, display\n",
        "display(Image('data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/per_layer_analysis.png'))\n"
      ],
      "metadata": {
        "id": "0fBAnIcXCkih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate best probe on test split\n",
        "!python scripts/eval_ood.py \\\n",
        "    --best_probe_json data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/best_probe.json \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --eval_dataset Deception-Roleplaying \\\n",
        "    --eval_split test\n",
        "\n",
        "# View test results\n",
        "!cat data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/eval_Deception-Roleplaying_test.json\n"
      ],
      "metadata": {
        "id": "In-HVL34GKHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Max pooling\n",
        "!python scripts/train_deception_probes.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --pooling max \\\n",
        "    --epochs 10 \\\n",
        "    --batch_size 32\n",
        "\n",
        "\n",
        "# Last token pooling\n",
        "!python scripts/train_deception_probes.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --pooling last \\\n",
        "    --epochs 10 \\\n",
        "    --batch_size 32\n",
        "\n",
        "# Attention pooling (learned)\n",
        "!python scripts/train_deception_probes.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --pooling attn \\\n",
        "    --batch_size 32 \\\n",
        "    --epochs 20\n"
      ],
      "metadata": {
        "id": "1q3a0YELIFZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Re-run compare_results.py with verbose output\n",
        "# ============================================================================\n",
        "\n",
        "!python scripts/compare_results.py \\\n",
        "    --experiments_dir data/probes \\\n",
        "    --output_dir results/comparisons \\\n",
        "    --save_csv 2>&1 | tee compare_output.txt\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìÅ Files generated:\")\n",
        "!ls -la results/comparisons/\n",
        "\n"
      ],
      "metadata": {
        "id": "uXbcbyWnIXhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Generate Layerwise Comparison Plot (Inline - No Script Needed)\n",
        "# ============================================================================\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PROBES_BASE = \"data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying\"\n",
        "OUTPUT_DIR = \"results/comparisons\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load all pooling results\n",
        "colors = {\n",
        "    'mean': '#2E86AB',\n",
        "    'max': '#A23B72',\n",
        "    'last': '#F18F01',\n",
        "    'attn': '#06A77D'\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    results_file = f\"{PROBES_BASE}/{pooling}/layer_results.json\"\n",
        "    if os.path.exists(results_file):\n",
        "        with open(results_file, 'r') as f:\n",
        "            all_results[pooling] = json.load(f)\n",
        "        print(f\"‚úì Loaded {pooling}\")\n",
        "\n",
        "# Check if accuracy data exists\n",
        "has_accuracy = all_results and 'val_acc' in all_results[list(all_results.keys())[0]][0]\n",
        "\n",
        "# Create figure\n",
        "if has_accuracy:\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
        "else:\n",
        "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
        "    ax2 = None\n",
        "\n",
        "overall_best_auc = 0\n",
        "overall_best_info = None\n",
        "\n",
        "# Plot each pooling strategy\n",
        "for pooling, layer_results in all_results.items():\n",
        "    layers = [r['layer'] for r in layer_results]\n",
        "    aucs = [r['val_auc'] for r in layer_results]\n",
        "    color = colors.get(pooling, '#666666')\n",
        "\n",
        "    # Plot AUC\n",
        "    ax1.plot(layers, aucs, marker='o', linewidth=2.5, markersize=6,\n",
        "             color=color, label=f'{pooling.upper()}', alpha=0.85)\n",
        "\n",
        "    # Mark best layer\n",
        "    best = max(layer_results, key=lambda x: x['val_auc'])\n",
        "    ax1.scatter([best['layer']], [best['val_auc']],\n",
        "                color=color, s=200, zorder=5, edgecolors='black',\n",
        "                linewidths=2.5, marker='*')\n",
        "\n",
        "    # Track overall best\n",
        "    if best['val_auc'] > overall_best_auc:\n",
        "        overall_best_auc = best['val_auc']\n",
        "        overall_best_info = (pooling, best['layer'], best['val_auc'])\n",
        "\n",
        "    # Plot accuracy if available\n",
        "    if ax2 is not None and 'val_acc' in layer_results[0]:\n",
        "        accs = [r.get('val_acc', 0.5) for r in layer_results]\n",
        "        ax2.plot(layers, accs, marker='s', linewidth=2.5, markersize=6,\n",
        "                 color=color, label=f'{pooling.upper()}', alpha=0.85)\n",
        "\n",
        "# Annotate overall best\n",
        "if overall_best_info:\n",
        "    pooling, layer, auc = overall_best_info\n",
        "    color = colors.get(pooling, '#666666')\n",
        "    ax1.annotate(\n",
        "        f'BEST: {pooling.upper()}\\nLayer {layer}\\nAUC: {auc:.3f}',\n",
        "        xy=(layer, auc),\n",
        "        xytext=(15, 15),\n",
        "        textcoords='offset points',\n",
        "        bbox=dict(boxstyle='round,pad=0.8', facecolor=color, alpha=0.3,\n",
        "                 edgecolor='black', linewidth=2),\n",
        "        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3',\n",
        "                       color='black', lw=2),\n",
        "        fontsize=11, fontweight='bold', ha='left'\n",
        "    )\n",
        "\n",
        "# Style AUC plot\n",
        "ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.4, linewidth=1.5, label='Random')\n",
        "ax1.axhline(y=0.7, color='green', linestyle=':', alpha=0.4, linewidth=1.5, label='Strong (0.7)')\n",
        "ax1.set_ylabel('Validation AUC', fontsize=13, fontweight='bold')\n",
        "ax1.set_title('Layerwise Validation AUC Comparison\\nAll 4 Pooling Strategies', fontsize=14, fontweight='bold')\n",
        "ax1.legend(loc='best', fontsize=11, framealpha=0.9)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "ax1.set_ylim(0.45, 1.0)\n",
        "\n",
        "# Style accuracy plot if present\n",
        "if ax2 is not None:\n",
        "    ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.4, linewidth=1.5)\n",
        "    ax2.set_xlabel('Layer', fontsize=13, fontweight='bold')\n",
        "    ax2.set_ylabel('Validation Accuracy', fontsize=13, fontweight='bold')\n",
        "    ax2.set_title('Layerwise Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(loc='best', fontsize=11, framealpha=0.9)\n",
        "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax2.set_ylim(0.45, 1.0)\n",
        "else:\n",
        "    ax1.set_xlabel('Layer', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save and display\n",
        "save_path = f\"{OUTPUT_DIR}/layerwise_pooling_comparison.png\"\n",
        "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"\\n‚úì Saved: {save_path}\")\n",
        "\n",
        "# Display\n",
        "from IPython.display import Image, display\n",
        "display(Image(save_path, width=900))\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä Summary: Best Layer for Each Pooling Strategy\")\n",
        "print(\"=\" * 60)\n",
        "for pooling, layer_results in all_results.items():\n",
        "    best = max(layer_results, key=lambda x: x['val_auc'])\n",
        "    marker = \" ‚≠ê BEST\" if overall_best_info and pooling == overall_best_info[0] else \"\"\n",
        "    print(f\"  {pooling.upper():6s}: Layer {best['layer']:2d} | AUC: {best['val_auc']:.4f}{marker}\")"
      ],
      "metadata": {
        "id": "gBtBpVTr7zrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and visualize per-layer results\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load results\n",
        "with open('data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/mean/layer_results.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "# Extract data\n",
        "layers = [r['layer'] for r in results]\n",
        "aucs = [r['val_auc'] for r in results]\n",
        "epochs = [r['epoch'] for r in results]\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: AUC per layer\n",
        "ax1.plot(layers, aucs, marker='o', linewidth=2, markersize=6, color='#2E86AB')\n",
        "ax1.axhline(y=0.5, color='red', linestyle='--', label='Random Chance', alpha=0.5)\n",
        "best_layer = max(results, key=lambda x: x['val_auc'])\n",
        "ax1.scatter([best_layer['layer']], [best_layer['val_auc']],\n",
        "            color='orange', s=200, zorder=5, label=f\"Best: Layer {best_layer['layer']}\")\n",
        "ax1.set_xlabel('Layer', fontsize=12)\n",
        "ax1.set_ylabel('Validation AUC', fontsize=12)\n",
        "ax1.set_title('Deception Detection by Layer', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: Training epochs per layer\n",
        "ax2.bar(layers, epochs, alpha=0.7, color='#A23B72')\n",
        "ax2.set_xlabel('Layer', fontsize=12)\n",
        "ax2.set_ylabel('Training Epochs', fontsize=12)\n",
        "ax2.set_title('Early Stopping Epochs by Layer', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('custom_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Best Layer: {best_layer['layer']} (AUC: {best_layer['val_auc']:.4f})\")\n"
      ],
      "metadata": {
        "id": "b_ik3wKYN-UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O1: Switch to main branch (SAFE for your data!)\n",
        "# ============================================================================\n",
        "%cd /content/drive/MyDrive/Efficacy-of-ensemble-of-attention-probes\n",
        "\n",
        "# Show what's in .gitignore (your data is protected)\n",
        "print(\"üìã Files protected by .gitignore:\")\n",
        "!grep -E \"^data/|^results/\" .gitignore\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Fetch and switch (ONLY affects code, not data)\n",
        "!git fetch origin\n",
        "!git checkout main\n",
        "!git pull origin main\n",
        "\n",
        "# Verify data still exists\n",
        "print(\"\\nüìÅ Verifying your data is still intact:\")\n",
        "!ls data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/\n",
        "\n",
        "# Verify InsiderTrading loader is present\n",
        "print(\"\\nüìã Checking for InsiderTrading loader:\")\n",
        "!grep -n \"InsiderTrading\" scripts/cache_deception_activations.py | head -3"
      ],
      "metadata": {
        "id": "OA7CH0ofOK_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O1: Switch to main branch to get Insider Trading dataset loader\n",
        "# ============================================================================\n",
        "%cd /content/drive/MyDrive/Efficacy-of-ensemble-of-attention-probes\n",
        "\n",
        "# Fetch latest and switch to main\n",
        "!git fetch origin\n",
        "!git checkout main\n",
        "!git pull origin main\n",
        "\n",
        "# Verify the InsiderTrading loader is present\n",
        "!grep -n \"InsiderTrading\" scripts/cache_deception_activations.py | head -3\n",
        "!grep -n \"InsiderTrading\" actprobe/src/actprobe/datasets/deception_loaders.py | head -3\n",
        "\n",
        "print(\"\\n‚úì Should see 'DeceptionInsiderTradingDataset' in the output above\")"
      ],
      "metadata": {
        "id": "2PnW9mMMuBxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O2: Download Insider Trading data (if available from Apollo)\n",
        "# ============================================================================\n",
        "\n",
        "# Try to download insider trading dataset\n",
        "!python scripts/download_apollo_data.py \\\n",
        "    --datasets insider_trading \\\n",
        "    --output_dir data/apollo_raw\n",
        "\n",
        "# Check if it was downloaded\n",
        "!ls -la data/apollo_raw/insider_trading/ 2>/dev/null || echo \"Note: If no file found, synthetic scenarios will be auto-generated\""
      ],
      "metadata": {
        "id": "dIITe1EQ9nXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Parse Apollo Insider Trading Data -> Create scenarios.json\n",
        "# ============================================================================\n",
        "import json\n",
        "import os\n",
        "\n",
        "input_file = \"data/apollo_raw/insider_trading/llama-70b-3.3-generations.json\"\n",
        "output_file = \"data/apollo_raw/insider_trading/scenarios.json\"\n",
        "\n",
        "print(\"üìÇ Loading Apollo insider trading data...\")\n",
        "with open(input_file, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"‚úì Loaded {len(data)} entries\")\n",
        "\n",
        "# Extract scenarios from the transcript format\n",
        "scenarios = []\n",
        "for i, entry in enumerate(data):\n",
        "    transcript = entry.get('transcript', [])\n",
        "\n",
        "    # Find the system message (contains the scenario)\n",
        "    system_msg = None\n",
        "    for msg in transcript:\n",
        "        if msg.get('role') == 'system':\n",
        "            system_msg = msg.get('content', '')\n",
        "            break\n",
        "\n",
        "    if system_msg:\n",
        "        scenarios.append({\n",
        "            'id': f'insider_{i}',\n",
        "            'scenario': system_msg[:2000],  # Truncate if too long\n",
        "            'transcript': transcript\n",
        "        })\n",
        "\n",
        "print(f\"‚úì Extracted {len(scenarios)} scenarios\")\n",
        "\n",
        "# Save as scenarios.json\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(scenarios, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Saved to {output_file}\")\n",
        "\n",
        "# Preview first scenario\n",
        "print(\"\\nüìã First scenario preview:\")\n",
        "print(scenarios[0]['scenario'][:500])"
      ],
      "metadata": {
        "id": "5yrzf8uACn-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove old synthetic data\n",
        "!rm -rf data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/\n",
        "\n",
        "# Cache with Apollo's pre-generated & pre-labeled data\n",
        "!python scripts/cache_deception_activations.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-InsiderTrading \\\n",
        "    --split test \\\n",
        "    --limit 200 \\\n",
        "    --batch_size 4 \\\n",
        "    --hf_token $HF_TOKEN\n",
        "\n",
        "# The script will auto-detect pre-generated responses and show:\n",
        "# üìå Detected pre-generated responses with labels in dataset\n",
        "# ‚ÑπÔ∏è  Skipping model generation & Cerebras labeling\n",
        "# ‚ÑπÔ∏è  Using Apollo Research's pre-classified labels"
      ],
      "metadata": {
        "id": "UpcWF9Iq9q7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O4: Validate OOD Activations\n",
        "# ============================================================================\n",
        "\n",
        "OOD_DATASET = \"Deception-InsiderTrading\"\n",
        "OOD_ACTIVATIONS_DIR = f\"data/activations/meta-llama_Llama-3.2-3B-Instruct/{OOD_DATASET}/test\"\n",
        "\n",
        "# Validate\n",
        "!python scripts/validate_deception_data.py \\\n",
        "    --activations_dir {OOD_ACTIVATIONS_DIR}\n",
        "\n",
        "# Check files\n",
        "!ls -lh {OOD_ACTIVATIONS_DIR}/\n",
        "\n",
        "# Preview manifest\n",
        "!head -n 3 {OOD_ACTIVATIONS_DIR}/manifest.jsonl"
      ],
      "metadata": {
        "id": "cnWgIHK-9x4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# OOD Evaluation - Fixed Inline Version\n",
        "# ============================================================================\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from safetensors.torch import load_file\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.path.join(os.getcwd(), 'actprobe', 'src'))\n",
        "from actprobe.probes.models import LayerProbe\n",
        "\n",
        "# Paths\n",
        "OOD_DIR = \"data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/test\"\n",
        "PROBES_BASE = \"data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying\"\n",
        "OUTPUT_DIR = \"results/ood_evaluation\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load OOD data (FIXED: use manifest + shard keys directly)\n",
        "print(\"Loading OOD data...\")\n",
        "with open(f\"{OOD_DIR}/manifest.jsonl\", 'r') as f:\n",
        "    manifest = [json.loads(line) for line in f]\n",
        "\n",
        "shard = load_file(f\"{OOD_DIR}/shard_000.safetensors\")\n",
        "\n",
        "samples = []\n",
        "labels = []\n",
        "for entry in manifest:\n",
        "    eid = entry['id']\n",
        "    if eid in shard:\n",
        "        samples.append(shard[eid])  # (L, T, D)\n",
        "        labels.append(entry['label'])\n",
        "\n",
        "X = torch.stack(samples).float()  # (N, L, T, D)\n",
        "y = np.array(labels)\n",
        "print(f\"‚úì Loaded {len(X)} OOD samples\")\n",
        "print(f\"  Labels: {sum(y==0)} honest, {sum(y==1)} deceptive\")\n",
        "\n",
        "# Colors\n",
        "COLORS = {'mean': '#2E86AB', 'max': '#A23B72', 'last': '#F18F01', 'attn': '#06A77D'}\n",
        "\n",
        "# Evaluate all pooling strategies\n",
        "all_results = {}\n",
        "\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    probe_dir = f\"{PROBES_BASE}/{pooling}\"\n",
        "    probe_files = sorted(glob.glob(f\"{probe_dir}/probe_layer_*.pt\"))\n",
        "\n",
        "    if not probe_files:\n",
        "        print(f\"‚ö†Ô∏è No probes for {pooling}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nEvaluating {pooling.upper()} ({len(probe_files)} layers)...\")\n",
        "\n",
        "    D = X.shape[-1]\n",
        "    layer_results = []\n",
        "\n",
        "    for pf in tqdm(probe_files, desc=pooling):\n",
        "        layer_idx = int(pf.split('_')[-1].replace('.pt', ''))\n",
        "\n",
        "        # Load probe\n",
        "        probe = LayerProbe(input_dim=D, pooling_type=pooling).to(device)\n",
        "        probe.load_state_dict(torch.load(pf, map_location=device))\n",
        "        probe.eval()\n",
        "\n",
        "        # Predict\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(X), 16):\n",
        "                batch = X[i:i+16, layer_idx, :, :].to(device)\n",
        "                logits = probe(batch)\n",
        "                probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "                preds.extend(probs)\n",
        "\n",
        "        preds = np.array(preds)\n",
        "        try:\n",
        "            auc = roc_auc_score(y, preds)\n",
        "        except:\n",
        "            auc = 0.5\n",
        "        acc = accuracy_score(y, (preds > 0.5).astype(int))\n",
        "\n",
        "        layer_results.append({'layer': layer_idx, 'auc': auc, 'acc': acc})\n",
        "\n",
        "    best = max(layer_results, key=lambda x: x['auc'])\n",
        "    all_results[pooling] = {\n",
        "        'layers': [r['layer'] for r in layer_results],\n",
        "        'aucs': [r['auc'] for r in layer_results],\n",
        "        'accs': [r['acc'] for r in layer_results],\n",
        "        'best_layer': best['layer'],\n",
        "        'best_auc': best['auc']\n",
        "    }\n",
        "    print(f\"  Best: Layer {best['layer']} | AUC: {best['auc']:.4f}\")\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "for pooling, res in all_results.items():\n",
        "    color = COLORS[pooling]\n",
        "    ax.plot(res['layers'], res['aucs'], marker='o', linewidth=2.5,\n",
        "            color=color, label=pooling.upper(), alpha=0.85)\n",
        "    ax.scatter([res['best_layer']], [res['best_auc']],\n",
        "               color=color, s=200, zorder=5, edgecolors='black',\n",
        "               linewidths=2.5, marker='*')\n",
        "\n",
        "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.4, label='Random')\n",
        "ax.set_xlabel('Layer', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('OOD AUC', fontsize=13, fontweight='bold')\n",
        "ax.set_title('OOD Evaluation: Insider Trading\\nAll Pooling Strategies', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=11)\n",
        "ax.grid(True, alpha=0.3, linestyle='--')\n",
        "ax.set_ylim(0.4, 1.0)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_path = f\"{OUTPUT_DIR}/ood_layerwise_comparison.png\"\n",
        "plt.savefig(save_path, dpi=300)\n",
        "print(f\"\\n‚úì Saved: {save_path}\")\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(save_path, width=900))\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä OOD EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "best_overall = max(all_results.items(), key=lambda x: x[1]['best_auc'])\n",
        "for pooling, res in all_results.items():\n",
        "    marker = \" ‚≠ê\" if pooling == best_overall[0] else \"\"\n",
        "    print(f\"  {pooling.upper():6s}: Layer {res['best_layer']:2d} | AUC: {res['best_auc']:.4f}{marker}\")\n",
        ""
      ],
      "metadata": {
        "id": "qk8Ubd5E_Kwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O5: Evaluate Trained Probes on OOD Dataset\n",
        "# ============================================================================\n",
        "\n",
        "# Paths\n",
        "PROBES_BASE = \"data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying\"\n",
        "OOD_ACTIVATIONS = f\"data/activations/meta-llama_Llama-3.2-3B-Instruct/{OOD_DATASET}/test\"\n",
        "OOD_RESULTS = \"results/ood_evaluation\"\n",
        "\n",
        "!python scripts/evaluate_ood_all_pooling.py \\\n",
        "    --probes_base_dir {PROBES_BASE} \\\n",
        "    --ood_activations_dir {OOD_ACTIVATIONS} \\\n",
        "    --ood_dataset_name \"Insider Trading\" \\\n",
        "    --output_dir {OOD_RESULTS} \\\n",
        "    --save_logits\n",
        "\n",
        "print(f\"\\n‚úÖ OOD evaluation saved to: {OOD_RESULTS}\")"
      ],
      "metadata": {
        "id": "C7Np6ERk_UQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O6: Display OOD Results\n",
        "# ============================================================================\n",
        "from IPython.display import Image, display\n",
        "\n",
        "OOD_RESULTS = \"results/ood_evaluation\"\n",
        "\n",
        "# Display comparison plot (all 4 pooling strategies on OOD)\n",
        "plot_path = f\"{OOD_RESULTS}/ood_layerwise_comparison.png\"\n",
        "print(\"üìä OOD Layerwise Comparison (All 4 Pooling Strategies):\")\n",
        "display(Image(plot_path, width=800))\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nüìã OOD Best Probes Summary:\")\n",
        "!cat {OOD_RESULTS}/ood_best_probes_summary.txt"
      ],
      "metadata": {
        "id": "EMdoLy1B71UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O7: Ensemble K-Sweep on Validation Set (All Pooling Strategies)\n",
        "# ============================================================================\n",
        "\n",
        "VAL_ACTIVATIONS = \"data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation\"\n",
        "PROBES_BASE = \"data/probes/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying\"\n",
        "ENSEMBLE_RESULTS = \"results/ensembles\"\n",
        "K_VALUES = \"10,20,30,40,50,60,70,80,90\"\n",
        "\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä Ensemble evaluation: {pooling.upper()} pooling (Validation)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    !python scripts/evaluate_ensembles_comprehensive.py \\\n",
        "        --pooling {pooling} \\\n",
        "        --val_activations_dir {VAL_ACTIVATIONS} \\\n",
        "        --probes_dir {PROBES_BASE}/{pooling} \\\n",
        "        --output_dir {ENSEMBLE_RESULTS}/{pooling} \\\n",
        "        --eval_mode validation \\\n",
        "        --k_values {K_VALUES}\n",
        "\n",
        "print(\"\\n‚úÖ Validation ensemble evaluation complete!\")"
      ],
      "metadata": {
        "id": "lgcfd-6173Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O8: Ensemble K-Sweep on OOD Set (All Pooling Strategies)\n",
        "# ============================================================================\n",
        "\n",
        "OOD_LOGITS_DIR = \"results/ood_evaluation/logits\"\n",
        "\n",
        "for pooling in ['mean', 'max', 'last', 'attn']:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä Ensemble evaluation: {pooling.upper()} pooling (OOD)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    logits_path = f\"{OOD_LOGITS_DIR}/{pooling}_logits.npy\"\n",
        "    labels_path = f\"{OOD_LOGITS_DIR}/labels.npy\"\n",
        "\n",
        "    !python scripts/evaluate_ensembles_comprehensive.py \\\n",
        "        --pooling {pooling} \\\n",
        "        --probes_dir {PROBES_BASE}/{pooling} \\\n",
        "        --ood_logits_path {logits_path} \\\n",
        "        --ood_labels_path {labels_path} \\\n",
        "        --output_dir {ENSEMBLE_RESULTS}/{pooling} \\\n",
        "        --eval_mode ood \\\n",
        "        --k_values {K_VALUES}\n",
        "\n",
        "print(\"\\n‚úÖ OOD ensemble evaluation complete!\")"
      ],
      "metadata": {
        "id": "X_TdjfeC75G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O9: Final Cross-Pooling √ó Ensemble Comparison\n",
        "# ============================================================================\n",
        "\n",
        "FINAL_COMPARISON = \"results/final_comparison\"\n",
        "\n",
        "!python scripts/compare_all_pooling_ensembles.py \\\n",
        "    --results_dir {ENSEMBLE_RESULTS} \\\n",
        "    --output_dir {FINAL_COMPARISON} \\\n",
        "    --eval_type both\n",
        "\n",
        "print(f\"\\n‚úÖ Final comparison saved to: {FINAL_COMPARISON}\")"
      ],
      "metadata": {
        "id": "6-yf1HB6760F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O10: Display All Final Visualizations\n",
        "# ============================================================================\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "FINAL_DIR = \"results/final_comparison\"\n",
        "\n",
        "# 1. Heatmaps: Pooling √ó Ensemble\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä POOLING √ó ENSEMBLE HEATMAPS (Best AUC)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for eval_type in ['validation', 'ood']:\n",
        "    heatmap = f\"{FINAL_DIR}/pooling_ensemble_heatmap_{eval_type}.png\"\n",
        "    if os.path.exists(heatmap):\n",
        "        print(f\"\\n{eval_type.upper()} Set:\")\n",
        "        display(Image(heatmap, width=600))\n",
        "\n",
        "# 2. Optimal K% Analysis\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä OPTIMAL K% ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for eval_type in ['validation', 'ood']:\n",
        "    k_plot = f\"{FINAL_DIR}/optimal_k_analysis_{eval_type}.png\"\n",
        "    if os.path.exists(k_plot):\n",
        "        print(f\"\\n{eval_type.upper()} Set:\")\n",
        "        display(Image(k_plot, width=800))\n",
        "\n",
        "# 3. Per-Ensemble Comparison\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä PER-ENSEMBLE COMPARISON (All Pooling Strategies)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for ensemble in ['mean', 'weighted', 'gated']:\n",
        "    for eval_type in ['validation', 'ood']:\n",
        "        plot = f\"{FINAL_DIR}/{ensemble}_comparison_{eval_type}.png\"\n",
        "        if os.path.exists(plot):\n",
        "            print(f\"\\n{ensemble.capitalize()} Ensemble - {eval_type.upper()}:\")\n",
        "            display(Image(plot, width=900))\n",
        "\n",
        "# 4. Final Summaries\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìã FINAL SUMMARIES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for eval_type in ['validation', 'ood']:\n",
        "    summary = f\"{FINAL_DIR}/final_summary_{eval_type}.txt\"\n",
        "    if os.path.exists(summary):\n",
        "        print(f\"\\n--- {eval_type.upper()} ---\")\n",
        "        !cat {summary}"
      ],
      "metadata": {
        "id": "hBYfboY-78Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O11: (Optional) PCA Visualization - ID vs OOD\n",
        "# ============================================================================\n",
        "\n",
        "!python scripts/analysis/analyze_pca.py \\\n",
        "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
        "    --dataset Deception-Roleplaying \\\n",
        "    --compare_dataset Deception-InsiderTrading \\\n",
        "    --data_dir data/activations \\\n",
        "    --output_dir results/pca \\\n",
        "    --layer 20 \\\n",
        "    --pooling mean\n",
        "\n",
        "# Display PCA plots\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "pca_dir = \"results/pca\"\n",
        "for f in sorted(os.listdir(pca_dir)):\n",
        "    if f.endswith('.png') or f.endswith('.pdf'):\n",
        "        print(f\"\\nüìä {f}:\")\n",
        "        display(Image(f\"{pca_dir}/{f}\", width=600))"
      ],
      "metadata": {
        "id": "REcuqDj-7-Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL O12: Ensure Results are Saved to Google Drive\n",
        "# ============================================================================\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Your results are already in Google Drive (since we're working in Drive)\n",
        "# But let's create a summary of what was generated\n",
        "\n",
        "print(\"üìÅ Results Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results_dirs = [\n",
        "    \"results/ood_evaluation\",\n",
        "    \"results/ensembles\",\n",
        "    \"results/final_comparison\",\n",
        "    \"results/pca\"\n",
        "]\n",
        "\n",
        "for d in results_dirs:\n",
        "    if os.path.exists(d):\n",
        "        files = os.listdir(d)\n",
        "        print(f\"\\nüìÇ {d}:\")\n",
        "        for f in sorted(files)[:10]:  # Show first 10 files\n",
        "            print(f\"   ‚Ä¢ {f}\")\n",
        "        if len(files) > 10:\n",
        "            print(f\"   ... and {len(files)-10} more files\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ All results are saved in your Google Drive!\")\n",
        "print(\"üìç Location: /content/drive/MyDrive/Efficacy-of-ensemble-of-attention-probes/results/\")"
      ],
      "metadata": {
        "id": "gHjfmr6Q8AJv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}