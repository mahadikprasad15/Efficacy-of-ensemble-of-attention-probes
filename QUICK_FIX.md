# üöÄ Quick Fix: "Loaded 0 OOD samples" Error

## ‚úÖ Problem FIXED!

The error was caused by a **data format mismatch** between the caching script and evaluation script.

### What was wrong:
- **Activations are saved**: `roleplaying_0` in safetensors + labels in `manifest.jsonl`
- **Script was expecting**: Keys ending in `_activations` and `_label` in safetensors

### What I fixed:
- ‚úÖ Updated `evaluate_ood_all_pooling.py` to read from `manifest.jsonl`
- ‚úÖ Updated `diagnose_activations.py` to show correct format info
- ‚úÖ Script now loads labels from manifest instead of safetensors

---

## üéØ Ready to Run!

Your evaluation should now work. Try running:

```python
# Run diagnostic first to verify everything is correct
!python scripts/diagnose_activations.py \
    --activations_dir data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation

# Then run the evaluation
!python scripts/evaluate_ood_all_pooling.py \
    --ood_activations data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation \
    --probes_base data/probes_flipped/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading \
    --output_dir results_flipped/ood_evaluation
```

---

## üìä What the Diagnostic Output Should Show

You should see:

```
==========================================================================================
ACTIVATION DIRECTORY DIAGNOSTIC
==========================================================================================
Directory: data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation

‚úì Directory exists

üìÅ Found 2 total files/directories:
   manifest.jsonl (0.06 MB)
   shard_000.safetensors (777.01 MB)

‚úì Found 1 shard files:
   shard_000.safetensors (777.01 MB)

Checking first shard: shard_000.safetensors
‚úì Successfully loaded shard

Keys in shard: 74
   roleplaying_0: shape=torch.Size([28, 64, 3072]), dtype=torch.float16
   ...

Samples in shard:
   Total tensor keys: 74

Key format analysis:
   Sample keys: ['roleplaying_0', 'roleplaying_108', ...]

‚úì Using NEW format (keys are sample IDs, labels in manifest.jsonl)
   This is the correct format for deception datasets!
   Tensor keys: 74

Sample activation shape: torch.Size([28, 64, 3072])
   Expected: (num_layers, num_tokens, hidden_dim)
   Got: torch.Size([28, 64, 3072])

Counting total samples across all shards...
‚úì Total tensors in shards: 74

‚úì Found manifest.jsonl
   Total entries: 74
   Honest (0): XX
   Deceptive (1): XX
   Unknown (-1): 0

‚úÖ Activations directory is valid with XX labeled samples!
```

---

## üé¨ Next Steps: Complete Evaluation Pipeline

Once the first command works, follow these steps:

### 1. OOD Evaluation (Already Working!)
```python
!python scripts/evaluate_ood_all_pooling.py \
    --ood_activations data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation \
    --probes_base data/probes_flipped/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading \
    --output_dir results_flipped/ood_evaluation
```

**Output**:
- `results_flipped/ood_evaluation/ood_results_all_pooling.json`
- `results_flipped/ood_evaluation/ood_layerwise_comparison.png`
- `results_flipped/ood_evaluation/logits/*.npy` (for ensembles)

### 2. Ensemble Evaluation (K-Sweep)

Run for each pooling strategy:

```python
# MEAN
!python scripts/evaluate_ensembles_comprehensive.py \
    --id_logits_dir results_flipped/ood_evaluation/logits \
    --ood_logits results_flipped/ood_evaluation/logits/mean_logits.npy \
    --ood_labels results_flipped/ood_evaluation/logits/labels.npy \
    --pooling_type mean \
    --output_dir results_flipped/ensembles/mean

# Repeat for: max, last, attn
```

### 3. Compare All Pooling Strategies

```python
!python scripts/compare_all_pooling_ensembles.py \
    --mean_results results_flipped/ensembles/mean/ensemble_k_sweep_ood.json \
    --max_results results_flipped/ensembles/max/ensemble_k_sweep_ood.json \
    --last_results results_flipped/ensembles/last/ensemble_k_sweep_ood.json \
    --attn_results results_flipped/ensembles/attn/ensemble_k_sweep_ood.json \
    --output_dir results_flipped/pooling_comparison
```

### 4. Gating Analysis (Fair Comparison)

```python
!python scripts/analyze_gating_weights.py \
    --id_activations data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/train \
    --ood_logits results_flipped/ood_evaluation/logits/attn_logits.npy \
    --ood_labels results_flipped/ood_evaluation/logits/labels.npy \
    --probes_dir data/probes_flipped/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/attn \
    --k_pct 40 \
    --output_dir results_flipped/gating_analysis
```

### 5. Visualizations

```python
# Attention analysis
!python scripts/analyze_attention_entropy.py \
    --activations data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/validation \
    --probes_dir data/probes_flipped/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/attn \
    --output_dir results_flipped/attention_analysis

# Token-level attention
!python scripts/analyze_attention_text.py \
    --activations data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/validation \
    --probes_dir data/probes_flipped/meta-llama_Llama-3.2-3B-Instruct/Deception-InsiderTrading/attn \
    --output_dir results_flipped/attention_text \
    --num_samples 10

# More visualizations...
# See COLAB_COMMANDS.md for complete list
```

---

## üìö Full Documentation

For complete documentation, see:
- **COLAB_COMMANDS.md** - All Colab commands with explanations
- **FLIPPED_EVALUATION_GUIDE.md** - Detailed guide with background

---

## üêõ Troubleshooting

### Still getting "0 samples loaded"?

1. **Check paths are correct**:
   ```python
   !ls -la data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying/validation/
   ```

2. **Verify files exist**:
   - Should see: `shard_*.safetensors` and `manifest.jsonl`
   - If not, you need to cache activations first

3. **Cache activations** (if needed):
   ```python
   !python scripts/cache_deception_activations.py \
       --model_name meta-llama/Llama-3.2-3B-Instruct \
       --dataset_name Deception-Roleplaying \
       --output_dir data/activations/meta-llama_Llama-3.2-3B-Instruct/Deception-Roleplaying \
       --split validation \
       --batch_size 4 \
       --max_samples 1000
   ```

### Out of memory?

Reduce batch size:
```python
!python scripts/evaluate_ood_all_pooling.py \
    --batch_size 16 \
    ... # other args
```

### Wrong paths in Colab?

Make sure you're in the project directory:
```python
%cd /content/drive/MyDrive/Efficacy-of-ensemble-of-attention-probes

# Then use relative paths
!python scripts/evaluate_ood_all_pooling.py \
    --ood_activations data/activations/...
```

---

## ‚úÖ Summary

The fix is now committed and ready to use! The evaluation script now correctly reads:
- ‚úÖ Activations from safetensors (tensor keys are sample IDs)
- ‚úÖ Labels from manifest.jsonl (JSON with id, label, metadata)

This matches the format used by `cache_deception_activations.py` and all other training/evaluation scripts.

**You're ready to run your flipped ID/OOD evaluation!** üöÄ
